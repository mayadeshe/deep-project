{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Vanilla DDPM Inpainting — Quantitative Evaluation (Light vs Heavy)\n",
    "\n",
    "This notebook evaluates the vanilla DDPM inpainting pipeline (`vanilla_second_try.py`) on **N images**\n",
    "from the **MS COCO val2017** dataset, using **MNIST digit silhouettes** as masks — in two modes:\n",
    "\n",
    "| Mode | Masked region | Approx. coverage |\n",
    "|------|--------------|-----------------|\n",
    "| **Light** | Digit silhouette (erased) | ~10% of image |\n",
    "| **Heavy** | Everything outside the digit (erased) | ~90% of image |\n",
    "\n",
    "**Pipeline:** For each image we use the COCO caption as the prompt and measure reconstruction quality\n",
    "(SSIM, PSNR, LPIPS) on the **inpainted region only**, then show metrics for both modes side-by-side.\n",
    "\n",
    "**Mask convention (`vanilla_second_try.py`):** `1 = keep, 0 = inpaint`.\n",
    "\n",
    "**Output layout:**\n",
    "```\n",
    "eval_results/\n",
    "  originals/           ← shared originals (written once)\n",
    "  light/inpainted/     ← light inpainting outputs\n",
    "  light/masks/         ← light masks (.pt)\n",
    "  heavy/inpainted/     ← heavy inpainting outputs\n",
    "  heavy/masks/         ← heavy masks (.pt)\n",
    "  checkpoint_light.json\n",
    "  checkpoint_heavy.json\n",
    "  metric_distributions_comparison.png\n",
    "  light_top10_best.png / light_top10_worst.png\n",
    "  heavy_top10_best.png / heavy_top10_worst.png\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-1-imports",
   "metadata": {},
   "source": [
    "import sys, os, json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim_fn\n",
    "from tqdm.auto import tqdm\n",
    "import lpips\n",
    "\n",
    "# ---- Config ----\n",
    "N_IMAGES       = 500       # Total images to evaluate\n",
    "IMAGE_SIZE     = (512, 512)\n",
    "SEED           = 42\n",
    "STEPS          = 50           # Reduce to 25 for ~2x speedup\n",
    "GUIDANCE_SCALE = 7.5\n",
    "DATA_ROOT      = \"./data\"\n",
    "RESULTS_DIR    = \"./eval_results\"\n",
    "\n",
    "# Modes: \"light\" = erase digit silhouette (~10% of image)\n",
    "#        \"heavy\" = erase everything except digit (~90% of image)\n",
    "MODES = [\"light\", \"heavy\"]\n",
    "\n",
    "CHECKPOINT_LIGHT = os.path.join(RESULTS_DIR, \"checkpoint_light.json\")\n",
    "CHECKPOINT_HEAVY = os.path.join(RESULTS_DIR, \"checkpoint_heavy.json\")\n",
    "\n",
    "COCO_ROOT      = os.path.join(DATA_ROOT, \"coco\")\n",
    "COCO_IMG_DIR   = os.path.join(COCO_ROOT, \"val2017\")\n",
    "COCO_ANN_FILE  = os.path.join(COCO_ROOT, \"annotations\", \"captions_val2017.json\")\n",
    "COCO_INST_FILE = os.path.join(COCO_ROOT, \"annotations\", \"instances_val2017.json\")\n",
    "\n",
    "# Shared originals dir (written once, used by both modes)\n",
    "os.makedirs(os.path.join(RESULTS_DIR, \"originals\"), exist_ok=True)\n",
    "# Per-mode output dirs\n",
    "for mode in MODES:\n",
    "    os.makedirs(os.path.join(RESULTS_DIR, mode, \"inpainted\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(RESULTS_DIR, mode, \"masks\"),     exist_ok=True)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(f\"Config: N_IMAGES={N_IMAGES}, SIZE={IMAGE_SIZE}, STEPS={STEPS}, GUIDANCE={GUIDANCE_SCALE}\")\n",
    "print(f\"Output dirs: {RESULTS_DIR}/{{light,heavy}}/{{inpainted,masks}}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-2-datasets",
   "metadata": {},
   "source": "import urllib.request\nimport zipfile\n\n# ---- reporthook for download progress ----\n_pbar = None\n\ndef reporthook(block_num, block_size, total_size):\n    global _pbar\n    if _pbar is None:\n        _pbar = tqdm(total=total_size, unit=\"B\", unit_scale=True, unit_divisor=1024)\n    downloaded = block_num * block_size\n    _pbar.n = min(downloaded, total_size)\n    _pbar.refresh()\n    if downloaded >= total_size:\n        _pbar.close()\n        _pbar = None\n\n# ---- Download COCO val2017 images (~1 GB) ----\nos.makedirs(COCO_ROOT, exist_ok=True)\n\nIMG_URL = \"http://images.cocodataset.org/zips/val2017.zip\"\nANN_URL = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n\nif not os.path.isdir(COCO_IMG_DIR):\n    img_zip = os.path.join(COCO_ROOT, \"val2017.zip\")\n    print(\"Downloading COCO val2017 images (~1 GB)...\")\n    urllib.request.urlretrieve(IMG_URL, img_zip, reporthook)\n    print(\"Extracting images...\")\n    with zipfile.ZipFile(img_zip) as z:\n        z.extractall(COCO_ROOT)\n    os.remove(img_zip)\n    print(\"Done.\")\nelse:\n    print(f\"COCO images already present: {COCO_IMG_DIR}\")\n\n# ---- Download COCO annotations (~240 MB) ----\nif not os.path.exists(COCO_ANN_FILE):\n    ann_zip = os.path.join(COCO_ROOT, \"annotations_trainval2017.zip\")\n    print(\"Downloading COCO annotations (~240 MB)...\")\n    urllib.request.urlretrieve(ANN_URL, ann_zip, reporthook)\n    print(\"Extracting annotations...\")\n    with zipfile.ZipFile(ann_zip) as z:\n        z.extractall(COCO_ROOT)\n    os.remove(ann_zip)\n    print(\"Done.\")\nelse:\n    print(f\"COCO annotations already present: {COCO_ANN_FILE}\")\n\n# ---- Load captions JSON → build img_id -> [captions] mapping ----\nwith open(COCO_ANN_FILE, \"r\") as f:\n    coco_data = json.load(f)\n\ncoco_id_to_filename = {img[\"id\"]: img[\"file_name\"] for img in coco_data[\"images\"]}\n\ncoco_captions = {}\nfor ann in coco_data[\"annotations\"]:\n    img_id = ann[\"image_id\"]\n    coco_captions.setdefault(img_id, []).append(ann[\"caption\"])\n\n# ---- Build set of image IDs to exclude (person, animal, sports, food) ----\n# These supercategories produce images dominated by foreground subjects rather\n# than buildings, rooms, scenery, nature, or street/vehicle scenes.\nEXCLUDED_SUPERCATEGORIES = {\"person\", \"animal\", \"sports\", \"food\"}\n\nwith open(COCO_INST_FILE, \"r\") as f:\n    inst_data = json.load(f)\n\nexcluded_cat_ids = {\n    c[\"id\"] for c in inst_data[\"categories\"]\n    if c[\"supercategory\"] in EXCLUDED_SUPERCATEGORIES\n}\nexcluded_image_ids = {\n    a[\"image_id\"] for a in inst_data[\"annotations\"]\n    if a[\"category_id\"] in excluded_cat_ids\n}\nprint(f\"Images excluded (person/animal/sports/food): \"\n      f\"{len(excluded_image_ids)} / {len(coco_id_to_filename)}\")\n\n# ---- Keep only scene/room/nature/vehicle images that have a caption and exist on disk ----\nvalid_ids = [\n    img_id for img_id, caps in coco_captions.items()\n    if img_id not in excluded_image_ids\n    and os.path.exists(os.path.join(COCO_IMG_DIR, coco_id_to_filename[img_id]))\n]\nprint(f\"Scene/room/nature/vehicle images remaining: {len(valid_ids)}\")\n\n# ---- Sample N_IMAGES image IDs deterministically ----\nrng = np.random.RandomState(SEED)\nsampled_ids = rng.choice(valid_ids, size=N_IMAGES, replace=False).tolist()\n\n# Build ordered list of (filename, first_caption) for the sampled images\ncoco_samples = [\n    (coco_id_to_filename[img_id], coco_captions[img_id][0])\n    for img_id in sampled_ids\n]\n\n# ---- Load MNIST (test set) ----\nmnist = torchvision.datasets.MNIST(\n    root=DATA_ROOT, train=False, download=True\n)\n\n# ---- Sample MNIST indices deterministically ----\nmnist_indices = rng.choice(len(mnist), size=N_IMAGES, replace=False)\n\nprint(f\"MNIST dataset size:  {len(mnist)}\")\nprint(f\"Sampled {N_IMAGES} scene/room/nature/vehicle image-mask pairs.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-3-mask-prep",
   "metadata": {},
   "source": "def prepare_mnist_mask(mnist_image: Image.Image, size=IMAGE_SIZE) -> torch.Tensor:\n    \"\"\"\n    Light inpainting mask: digit silhouette is the erased region (~10% of image).\n    Convention: 1 = keep, 0 = inpaint.\n    Digit pixels (bright >127) -> 0 (inpaint), background -> 1 (keep).\n\n    Returns: torch.Tensor of shape (1, 1, H, W)\n    \"\"\"\n    mask_pil = mnist_image.resize(size, Image.NEAREST)\n    mask_np = np.array(mask_pil)\n    # Digit pixels are bright; invert so digit -> 0 (inpaint)\n    mask_binary = (mask_np <= 127).astype(np.float32)\n    mask_tensor = torch.from_numpy(mask_binary).unsqueeze(0).unsqueeze(0)\n    return mask_tensor\n\n\ndef prepare_mnist_mask_heavy(mnist_image: Image.Image, size=IMAGE_SIZE) -> torch.Tensor:\n    \"\"\"\n    Heavy inpainting mask: everything OUTSIDE the digit is erased (~90% of image).\n    Convention: 1 = keep, 0 = inpaint.\n    Digit pixels (bright >127) -> 1 (keep), background -> 0 (inpaint).\n\n    This is the inverse of prepare_mnist_mask — the digit is the only known region.\n\n    Returns: torch.Tensor of shape (1, 1, H, W)\n    \"\"\"\n    mask_pil = mnist_image.resize(size, Image.NEAREST)\n    mask_np = np.array(mask_pil)\n    # Digit pixels are bright; keep digit -> 1, erase background -> 0\n    mask_binary = (mask_np > 127).astype(np.float32)\n    mask_tensor = torch.from_numpy(mask_binary).unsqueeze(0).unsqueeze(0)\n    return mask_tensor\n\n\ndef prepare_coco_image(path: str, size=IMAGE_SIZE) -> Image.Image:\n    \"\"\"Load a COCO image from disk and resize to the target size.\"\"\"\n    return Image.open(path).convert(\"RGB\").resize(size, Image.LANCZOS)\n\n\ndef apply_mask_for_display(image: Image.Image, mask_tensor: torch.Tensor) -> Image.Image:\n    \"\"\"Black out the inpainted region for visualization.\"\"\"\n    img_np = np.array(image).copy()\n    mask_np = mask_tensor.squeeze().numpy()  # (H, W), 1=keep, 0=inpaint\n    img_np[mask_np == 0] = 0  # black for inpainted region\n    return Image.fromarray(img_np)\n\n\n# ---- Sanity check: display one example for BOTH modes ----\nsample_filename, sample_caption = coco_samples[0]\nsample_img     = prepare_coco_image(os.path.join(COCO_IMG_DIR, sample_filename))\nsample_digit   = mnist[mnist_indices[0]][0]\n\nsample_mask_light = prepare_mnist_mask(sample_digit)\nsample_mask_heavy = prepare_mnist_mask_heavy(sample_digit)\n\nsample_masked_light = apply_mask_for_display(sample_img, sample_mask_light)\nsample_masked_heavy = apply_mask_for_display(sample_img, sample_mask_heavy)\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# --- Light row ---\naxes[0, 0].imshow(sample_img);                                   axes[0, 0].set_title(\"Original (COCO)\")\naxes[0, 1].imshow(sample_mask_light.squeeze(), cmap=\"gray\");     axes[0, 1].set_title(\"Light mask (1=keep)\")\naxes[0, 2].imshow(sample_masked_light);                          axes[0, 2].set_title(\"Masked — light (digit erased)\")\naxes[0, 0].set_ylabel(\"Light\\n(erase digit)\", fontsize=11, rotation=0, labelpad=60, va=\"center\")\n\n# --- Heavy row ---\naxes[1, 0].imshow(sample_img);                                   axes[1, 0].set_title(\"\")\naxes[1, 1].imshow(sample_mask_heavy.squeeze(), cmap=\"gray\");     axes[1, 1].set_title(\"Heavy mask (1=keep)\")\naxes[1, 2].imshow(sample_masked_heavy);                          axes[1, 2].set_title(\"Masked — heavy (background erased)\")\naxes[1, 0].set_ylabel(\"Heavy\\n(erase background)\", fontsize=11, rotation=0, labelpad=60, va=\"center\")\n\nfor ax in axes.flat:\n    ax.axis(\"off\")\n\nplt.suptitle(f'Caption: \"{sample_caption}\"', fontsize=10, y=1.01)\nplt.tight_layout()\nplt.show()\n\nlight_frac = (sample_mask_light == 0).float().mean().item() * 100\nheavy_frac = (sample_mask_heavy == 0).float().mean().item() * 100\nprint(f\"Light inpainted region: {light_frac:.1f}% of image\")\nprint(f\"Heavy inpainted region: {heavy_frac:.1f}% of image\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-4-model",
   "metadata": {},
   "source": "# ---- Import model and inpainting function from vanilla_second_try.py ----\nsys.path.insert(0, os.path.abspath(\".\"))\nfrom vanilla_second_try import load_vanilla_model, ddpm_inpaint\n\ndevice = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\nprint(\"Loading model...\")\npipe = load_vanilla_model(device)\npipe.set_progress_bar_config(disable=True)  # suppress inner per-step bar\nprint(\"Model loaded.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-5-inpaint-loop",
   "metadata": {},
   "source": "# ---- LIGHT Inpainting loop (erase digit silhouette ~10% of image) ----\n# Saves to: eval_results/originals/, eval_results/light/inpainted/, eval_results/light/masks/\n\nLIGHT_INPAINTED_DIR = os.path.join(RESULTS_DIR, \"light\", \"inpainted\")\nLIGHT_MASKS_DIR     = os.path.join(RESULTS_DIR, \"light\", \"masks\")\nORIGINALS_DIR       = os.path.join(RESULTS_DIR, \"originals\")\n\n# Check for existing checkpoint\nstart_idx = 0\nif os.path.exists(CHECKPOINT_LIGHT):\n    with open(CHECKPOINT_LIGHT, \"r\") as f:\n        checkpoint = json.load(f)\n    start_idx = checkpoint.get(\"completed\", 0)\n    print(f\"[Light] Resuming from image {start_idx}/{N_IMAGES}\")\n\nfor i in tqdm(range(start_idx, N_IMAGES), initial=start_idx,\n              total=N_IMAGES, desc=\"Light inpainting\"):\n    filename, prompt = coco_samples[i]\n    img_pil = prepare_coco_image(os.path.join(COCO_IMG_DIR, filename))\n    mask_tensor = prepare_mnist_mask(mnist[mnist_indices[i]][0])\n\n    inpainted_pil = ddpm_inpaint(\n        pipe=pipe,\n        image=img_pil,\n        mask=mask_tensor,\n        prompt=prompt,\n        steps=STEPS,\n        guidance_scale=GUIDANCE_SCALE,\n        seed=SEED + i,\n    )\n\n    # Save originals once (shared between modes)\n    orig_path = os.path.join(ORIGINALS_DIR, f\"{i:04d}.png\")\n    if not os.path.exists(orig_path):\n        img_pil.save(orig_path)\n        with open(os.path.join(ORIGINALS_DIR, f\"{i:04d}.json\"), \"w\") as f:\n            json.dump({\"caption\": prompt}, f)\n\n    inpainted_pil.save(os.path.join(LIGHT_INPAINTED_DIR, f\"{i:04d}.png\"))\n    torch.save(mask_tensor, os.path.join(LIGHT_MASKS_DIR, f\"{i:04d}.pt\"))\n\n    with open(CHECKPOINT_LIGHT, \"w\") as f:\n        json.dump({\"completed\": i + 1, \"last_prompt\": prompt}, f)\n\nprint(f\"[Light] Done. Inpainted {N_IMAGES} images.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ntho99nby9l",
   "source": "# ---- HEAVY Inpainting loop (erase background ~90% of image, keep only digit) ----\n# Uses the same COCO image, MNIST digit, prompt, and seed as the corresponding light run.\n# Saves to: eval_results/heavy/inpainted/, eval_results/heavy/masks/\n\nHEAVY_INPAINTED_DIR = os.path.join(RESULTS_DIR, \"heavy\", \"inpainted\")\nHEAVY_MASKS_DIR     = os.path.join(RESULTS_DIR, \"heavy\", \"masks\")\n\n# Check for existing checkpoint\nstart_idx_heavy = 0\nif os.path.exists(CHECKPOINT_HEAVY):\n    with open(CHECKPOINT_HEAVY, \"r\") as f:\n        checkpoint_h = json.load(f)\n    start_idx_heavy = checkpoint_h.get(\"completed\", 0)\n    print(f\"[Heavy] Resuming from image {start_idx_heavy}/{N_IMAGES}\")\n\nfor i in tqdm(range(start_idx_heavy, N_IMAGES), initial=start_idx_heavy,\n              total=N_IMAGES, desc=\"Heavy inpainting\"):\n    filename, prompt = coco_samples[i]\n    img_pil = prepare_coco_image(os.path.join(COCO_IMG_DIR, filename))\n    mask_tensor = prepare_mnist_mask_heavy(mnist[mnist_indices[i]][0])\n\n    inpainted_pil = ddpm_inpaint(\n        pipe=pipe,\n        image=img_pil,\n        mask=mask_tensor,\n        prompt=prompt,\n        steps=STEPS,\n        guidance_scale=GUIDANCE_SCALE,\n        seed=SEED + i,\n    )\n\n    # Originals already saved by the light loop; skip if present\n    orig_path = os.path.join(ORIGINALS_DIR, f\"{i:04d}.png\")\n    if not os.path.exists(orig_path):\n        img_pil.save(orig_path)\n        with open(os.path.join(ORIGINALS_DIR, f\"{i:04d}.json\"), \"w\") as f:\n            json.dump({\"caption\": prompt}, f)\n\n    inpainted_pil.save(os.path.join(HEAVY_INPAINTED_DIR, f\"{i:04d}.png\"))\n    torch.save(mask_tensor, os.path.join(HEAVY_MASKS_DIR, f\"{i:04d}.pt\"))\n\n    with open(CHECKPOINT_HEAVY, \"w\") as f:\n        json.dump({\"completed\": i + 1, \"last_prompt\": prompt}, f)\n\nprint(f\"[Heavy] Done. Inpainted {N_IMAGES} images.\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-6-metrics",
   "metadata": {},
   "source": "# ---- Metric helpers ----\n\nlpips_model = lpips.LPIPS(net=\"alex\").to(device)\nlpips_model.eval()\n\n\ndef compute_masked_metrics(\n    original: Image.Image,\n    inpainted: Image.Image,\n    mask_tensor: torch.Tensor,\n) -> dict:\n    \"\"\"\n    Compute SSIM, PSNR, LPIPS only on the inpainted region.\n    mask_tensor: (1,1,H,W), 1=keep, 0=inpaint.\n    \"\"\"\n    orig_np = np.array(original).astype(np.float64)\n    inp_np  = np.array(inpainted).astype(np.float64)\n    mask_np = mask_tensor.squeeze().numpy()\n    inpaint_mask = (mask_np == 0)\n\n    if not inpaint_mask.any():\n        return {\"ssim\": 1.0, \"psnr\": float(\"inf\"), \"lpips\": 0.0}\n\n    # ---- Bounding box of inpainted region ----\n    rows = np.any(inpaint_mask, axis=1)\n    cols = np.any(inpaint_mask, axis=0)\n    rmin, rmax = np.where(rows)[0][[0, -1]]\n    cmin, cmax = np.where(cols)[0][[0, -1]]\n\n    crop_orig = orig_np[rmin:rmax + 1, cmin:cmax + 1]\n    crop_inp  = inp_np[rmin:rmax + 1, cmin:cmax + 1]\n\n    # ---- PSNR: only on inpainted pixels ----\n    masked_orig = orig_np[inpaint_mask]\n    masked_inp  = inp_np[inpaint_mask]\n    mse = np.mean((masked_orig - masked_inp) ** 2)\n    psnr_val = 10.0 * np.log10(255.0 ** 2 / mse) if mse > 0 else float(\"inf\")\n\n    # ---- SSIM: on bounding-box crop ----\n    min_dim = min(crop_orig.shape[0], crop_orig.shape[1])\n    win_size = min(7, min_dim if min_dim % 2 == 1 else min_dim - 1)\n    win_size = max(win_size, 3)\n\n    ssim_val = ssim_fn(\n        crop_orig, crop_inp,\n        data_range=255.0, channel_axis=2, win_size=win_size\n    )\n\n    # ---- LPIPS: on bounding-box crop (avoids black-region artifacts) ----\n    crop_orig_f = crop_orig.astype(np.float32)\n    crop_inp_f  = crop_inp.astype(np.float32)\n\n    orig_t = torch.from_numpy(crop_orig_f).permute(2, 0, 1).unsqueeze(0) / 127.5 - 1.0\n    inp_t  = torch.from_numpy(crop_inp_f).permute(2, 0, 1).unsqueeze(0) / 127.5 - 1.0\n\n    with torch.no_grad():\n        lpips_val = lpips_model(orig_t.to(device), inp_t.to(device)).item()\n\n    return {\"ssim\": ssim_val, \"psnr\": psnr_val, \"lpips\": lpips_val}\n\n\ndef run_metrics(inpainted_dir: str, masks_dir: str, originals_dir: str) -> list:\n    \"\"\"\n    Load inpainted images, masks, and originals from disk and compute per-image metrics.\n    Returns a list of result dicts with keys: idx, original, inpainted, mask, caption, ssim, psnr, lpips.\n    \"\"\"\n    results = []\n    for i in tqdm(range(N_IMAGES), desc=f\"Metrics [{os.path.basename(inpainted_dir)}]\"):\n        orig_path    = os.path.join(originals_dir,  f\"{i:04d}.png\")\n        inp_path     = os.path.join(inpainted_dir,  f\"{i:04d}.png\")\n        mask_path    = os.path.join(masks_dir,       f\"{i:04d}.pt\")\n        caption_path = os.path.join(originals_dir,  f\"{i:04d}.json\")\n\n        if not all(os.path.exists(p) for p in [orig_path, inp_path, mask_path]):\n            print(f\"Stopping at image {i} (files not found). Run the inpainting loop first.\")\n            break\n\n        orig_img = Image.open(orig_path)\n        inp_img  = Image.open(inp_path)\n        mask_t   = torch.load(mask_path, weights_only=True)\n\n        caption = \"\"\n        if os.path.exists(caption_path):\n            with open(caption_path) as f:\n                caption = json.load(f).get(\"caption\", \"\")\n\n        metrics = compute_masked_metrics(orig_img, inp_img, mask_t)\n        results.append({\n            \"idx\": i, \"original\": orig_img, \"inpainted\": inp_img,\n            \"mask\": mask_t, \"caption\": caption, **metrics\n        })\n\n    return results\n\n\n# ---- Compute metrics for both modes ----\noriginals_dir = os.path.join(RESULTS_DIR, \"originals\")\n\nresults_light = run_metrics(\n    inpainted_dir=os.path.join(RESULTS_DIR, \"light\", \"inpainted\"),\n    masks_dir=os.path.join(RESULTS_DIR, \"light\", \"masks\"),\n    originals_dir=originals_dir,\n)\n\nresults_heavy = run_metrics(\n    inpainted_dir=os.path.join(RESULTS_DIR, \"heavy\", \"inpainted\"),\n    masks_dir=os.path.join(RESULTS_DIR, \"heavy\", \"masks\"),\n    originals_dir=originals_dir,\n)\n\nprint(f\"Light metrics computed for {len(results_light)} images.\")\nprint(f\"Heavy metrics computed for {len(results_heavy)} images.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-7-summary",
   "metadata": {},
   "source": "# ---- Aggregate statistics and side-by-side comparison ----\nfrom scipy.stats import gaussian_kde\n\ndef print_stats_table(results, label):\n    ssim_v  = np.array([r[\"ssim\"]  for r in results])\n    psnr_v  = np.array([r[\"psnr\"]  for r in results])\n    lpips_v = np.array([r[\"lpips\"] for r in results])\n    print(f\"\\n{'=' * 55}\")\n    print(f\"  {label} — {len(results)} images (inpainted region only)\")\n    print(f\"{'=' * 55}\")\n    print(f\"{'Metric':<10} {'Mean':>10} {'Median':>10} {'Std':>10}\")\n    print(f\"{'-' * 55}\")\n    print(f\"{'SSIM':<10} {ssim_v.mean():>10.4f} {np.median(ssim_v):>10.4f} {ssim_v.std():>10.4f}\")\n    print(f\"{'PSNR':<10} {psnr_v.mean():>10.2f} {np.median(psnr_v):>10.2f} {psnr_v.std():>10.2f}\")\n    print(f\"{'LPIPS':<10} {lpips_v.mean():>10.4f} {np.median(lpips_v):>10.4f} {lpips_v.std():>10.4f}\")\n    print(f\"{'=' * 55}\")\n    return ssim_v, psnr_v, lpips_v\n\nssim_l,  psnr_l,  lpips_l  = print_stats_table(results_light, \"LIGHT inpainting (erase digit ~10%)\")\nssim_h,  psnr_h,  lpips_h  = print_stats_table(results_heavy, \"HEAVY inpainting (erase background ~90%)\")\n\n# ---- Overlay KDE distributions: light vs heavy ----\nCOLORS = {\"light\": \"steelblue\", \"heavy\": \"tomato\"}\n\ndef plot_kde_overlay(ax, vals_a, vals_b, label_a, label_b, color_a, color_b, xlabel, title):\n    for vals, label, color in [(vals_a, label_a, color_a), (vals_b, label_b, color_b)]:\n        kde = gaussian_kde(vals, bw_method=\"scott\")\n        x = np.linspace(\n            min(vals_a.min(), vals_b.min()) - 0.05 * max(np.ptp(vals_a), np.ptp(vals_b)),\n            max(vals_a.max(), vals_b.max()) + 0.05 * max(np.ptp(vals_a), np.ptp(vals_b)),\n            500,\n        )\n        y = kde(x)\n        ax.plot(x, y, color=color, linewidth=2, label=f\"{label} (mean={vals.mean():.3f})\")\n        ax.fill_between(x, y, alpha=0.20, color=color)\n        ax.axvline(vals.mean(), color=color, linestyle=\"--\", linewidth=1.2)\n    ax.set_title(title)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(\"Density\")\n    ax.legend(fontsize=8)\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 4))\nplot_kde_overlay(axes[0], ssim_l,  ssim_h,  \"Light\", \"Heavy\",\n                 COLORS[\"light\"], COLORS[\"heavy\"], \"SSIM\",  \"SSIM Distribution\")\nplot_kde_overlay(axes[1], psnr_l,  psnr_h,  \"Light\", \"Heavy\",\n                 COLORS[\"light\"], COLORS[\"heavy\"], \"PSNR (dB)\", \"PSNR Distribution\")\nplot_kde_overlay(axes[2], lpips_l, lpips_h, \"Light\", \"Heavy\",\n                 COLORS[\"light\"], COLORS[\"heavy\"], \"LPIPS (lower is better)\", \"LPIPS Distribution\")\n\nplt.suptitle(\"Light vs Heavy Inpainting — Metric Distributions\", fontsize=13, y=1.02)\nplt.tight_layout()\nplt.savefig(os.path.join(RESULTS_DIR, \"metric_distributions_comparison.png\"), dpi=150, bbox_inches=\"tight\")\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-8-top10-best",
   "metadata": {},
   "source": "# ---- Visualization helper ----\n\ndef show_top10(results, sort_key=\"ssim\", descending=True, title_prefix=\"\", save_path=None):\n    \"\"\"Display and optionally save a 10-row grid: Original | Masked | Inpainted.\"\"\"\n    sorted_r = sorted(results, key=lambda r: r[sort_key], reverse=descending)\n    top10 = sorted_r[:10]\n\n    direction = \"highest\" if descending else \"lowest\"\n    fig, axes = plt.subplots(10, 3, figsize=(15, 50))\n    fig.suptitle(f\"{title_prefix} (Top 10 — {direction} {sort_key.upper()})\", fontsize=16, y=1.0)\n\n    for row, r in enumerate(top10):\n        masked_vis = apply_mask_for_display(r[\"original\"], r[\"mask\"])\n\n        axes[row, 0].imshow(r[\"original\"])\n        axes[row, 0].set_title(\"Original\" if row == 0 else \"\")\n        axes[row, 0].set_ylabel(\n            f\"#{r['idx']}\\nSSIM={r['ssim']:.3f}\\nPSNR={r['psnr']:.1f}\\nLPIPS={r['lpips']:.3f}\",\n            fontsize=9, rotation=0, labelpad=70, va=\"center\",\n        )\n\n        axes[row, 1].imshow(masked_vis)\n        axes[row, 1].set_title(\"Masked\" if row == 0 else \"\")\n        axes[row, 1].set_xlabel(f'\"{r[\"caption\"]}\"', fontsize=8, labelpad=6)\n\n        axes[row, 2].imshow(r[\"inpainted\"])\n        axes[row, 2].set_title(\"Inpainted\" if row == 0 else \"\")\n\n        for col in range(3):\n            axes[row, col].axis(\"off\")\n        axes[row, 1].xaxis.set_visible(True)\n        axes[row, 1].tick_params(bottom=False, labelbottom=True)\n        axes[row, 1].set_xticks([])\n\n    plt.tight_layout()\n    if save_path:\n        plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n    plt.show()\n\n\n# ---- Top 10 BEST — Light ----\nshow_top10(\n    results_light,\n    sort_key=\"ssim\", descending=True,\n    title_prefix=\"LIGHT Inpainting — Best Results\",\n    save_path=os.path.join(RESULTS_DIR, \"light_top10_best.png\"),\n)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-9-top10-worst",
   "metadata": {},
   "source": "# ---- Top 10 WORST — Light ----\nshow_top10(\n    results_light,\n    sort_key=\"ssim\", descending=False,\n    title_prefix=\"LIGHT Inpainting — Worst Results\",\n    save_path=os.path.join(RESULTS_DIR, \"light_top10_worst.png\"),)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2q5gl19iq8e",
   "source": "# ---- Top 10 BEST — Heavy ----\nshow_top10(\n    results_heavy,\n    sort_key=\"ssim\", descending=True,\n    title_prefix=\"HEAVY Inpainting — Best Results\",\n    save_path=os.path.join(RESULTS_DIR, \"heavy_top10_best.png\"),\n)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bmmqf3wgdbi",
   "source": "# ---- Top 10 WORST — Heavy ----\nshow_top10(\n    results_heavy,\n    sort_key=\"ssim\", descending=False,\n    title_prefix=\"HEAVY Inpainting — Worst Results\",\n    save_path=os.path.join(RESULTS_DIR, \"heavy_top10_worst.png\"),\n)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
