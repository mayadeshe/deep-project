{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Vanilla DDPM Inpainting — Quantitative Evaluation\n",
    "\n",
    "This notebook evaluates the vanilla DDPM inpainting pipeline (`vanilla_second_try.py`) on **1000 images**\n",
    "from the **MS COCO val2017** dataset, using **MNIST digit silhouettes** as masks.\n",
    "\n",
    "**Pipeline:** For each image we erase a digit-shaped region and ask the model to reconstruct it using the\n",
    "image's COCO caption as the prompt. We then measure reconstruction quality via different metrics (SSIM,\n",
    "PSNR, LPIPS) on the **masked region only** and display the 10 best and 10 worst results.\n",
    "\n",
    "**Mask convention (`vanilla_second_try.py`):** `1 = keep, 0 = inpaint`.\n",
    "\n",
    "**Limitations:** We decided to focus on "
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-1-imports",
   "metadata": {},
   "source": "import sys, os, json\nimport numpy as np\nimport torch\nimport torchvision\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom skimage.metrics import structural_similarity as ssim_fn\nfrom tqdm.auto import tqdm\n\ntry:\n    import lpips\nexcept ImportError:\n    import subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lpips\"])\n    import lpips\n\n# ---- Config ----\nN_IMAGES       = 20       # Total images to evaluate\nIMAGE_SIZE     = (512, 512)\nSEED           = 42\nSTEPS          = 50           # Reduce to 25 for ~2x speedup\nGUIDANCE_SCALE = 7.5\nDATA_ROOT      = \"./data\"\nRESULTS_DIR    = \"./eval_results\"\nCHECKPOINT_PATH = os.path.join(RESULTS_DIR, \"checkpoint.json\")\n\nCOCO_ROOT      = os.path.join(DATA_ROOT, \"coco\")\nCOCO_IMG_DIR   = os.path.join(COCO_ROOT, \"val2017\")\nCOCO_ANN_FILE  = os.path.join(COCO_ROOT, \"annotations\", \"captions_val2017.json\")\nCOCO_INST_FILE = os.path.join(COCO_ROOT, \"annotations\", \"instances_val2017.json\")\n\nos.makedirs(os.path.join(RESULTS_DIR, \"originals\"), exist_ok=True)\nos.makedirs(os.path.join(RESULTS_DIR, \"inpainted\"), exist_ok=True)\nos.makedirs(os.path.join(RESULTS_DIR, \"masks\"), exist_ok=True)\n\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\nprint(f\"Config: N_IMAGES={N_IMAGES}, SIZE={IMAGE_SIZE}, STEPS={STEPS}, GUIDANCE={GUIDANCE_SCALE}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-2-datasets",
   "metadata": {},
   "source": "import urllib.request\nimport zipfile\n\n# ---- reporthook for download progress ----\n_pbar = None\n\ndef reporthook(block_num, block_size, total_size):\n    global _pbar\n    if _pbar is None:\n        _pbar = tqdm(total=total_size, unit=\"B\", unit_scale=True, unit_divisor=1024)\n    downloaded = block_num * block_size\n    _pbar.n = min(downloaded, total_size)\n    _pbar.refresh()\n    if downloaded >= total_size:\n        _pbar.close()\n        _pbar = None\n\n# ---- Download COCO val2017 images (~1 GB) ----\nos.makedirs(COCO_ROOT, exist_ok=True)\n\nIMG_URL = \"http://images.cocodataset.org/zips/val2017.zip\"\nANN_URL = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n\nif not os.path.isdir(COCO_IMG_DIR):\n    img_zip = os.path.join(COCO_ROOT, \"val2017.zip\")\n    print(\"Downloading COCO val2017 images (~1 GB)...\")\n    urllib.request.urlretrieve(IMG_URL, img_zip, reporthook)\n    print(\"Extracting images...\")\n    with zipfile.ZipFile(img_zip) as z:\n        z.extractall(COCO_ROOT)\n    os.remove(img_zip)\n    print(\"Done.\")\nelse:\n    print(f\"COCO images already present: {COCO_IMG_DIR}\")\n\n# ---- Download COCO annotations (~240 MB) ----\nif not os.path.exists(COCO_ANN_FILE):\n    ann_zip = os.path.join(COCO_ROOT, \"annotations_trainval2017.zip\")\n    print(\"Downloading COCO annotations (~240 MB)...\")\n    urllib.request.urlretrieve(ANN_URL, ann_zip, reporthook)\n    print(\"Extracting annotations...\")\n    with zipfile.ZipFile(ann_zip) as z:\n        z.extractall(COCO_ROOT)\n    os.remove(ann_zip)\n    print(\"Done.\")\nelse:\n    print(f\"COCO annotations already present: {COCO_ANN_FILE}\")\n\n# ---- Load captions JSON → build img_id -> [captions] mapping ----\nwith open(COCO_ANN_FILE, \"r\") as f:\n    coco_data = json.load(f)\n\ncoco_id_to_filename = {img[\"id\"]: img[\"file_name\"] for img in coco_data[\"images\"]}\n\ncoco_captions = {}\nfor ann in coco_data[\"annotations\"]:\n    img_id = ann[\"image_id\"]\n    coco_captions.setdefault(img_id, []).append(ann[\"caption\"])\n\n# ---- Build set of image IDs to exclude (person, animal, sports, food) ----\n# These supercategories produce images dominated by foreground subjects rather\n# than buildings, rooms, scenery, nature, or street/vehicle scenes.\nEXCLUDED_SUPERCATEGORIES = {\"person\", \"animal\", \"sports\", \"food\"}\n\nwith open(COCO_INST_FILE, \"r\") as f:\n    inst_data = json.load(f)\n\nexcluded_cat_ids = {\n    c[\"id\"] for c in inst_data[\"categories\"]\n    if c[\"supercategory\"] in EXCLUDED_SUPERCATEGORIES\n}\nexcluded_image_ids = {\n    a[\"image_id\"] for a in inst_data[\"annotations\"]\n    if a[\"category_id\"] in excluded_cat_ids\n}\nprint(f\"Images excluded (person/animal/sports/food): \"\n      f\"{len(excluded_image_ids)} / {len(coco_id_to_filename)}\")\n\n# ---- Keep only scene/room/nature/vehicle images that have a caption and exist on disk ----\nvalid_ids = [\n    img_id for img_id, caps in coco_captions.items()\n    if img_id not in excluded_image_ids\n    and os.path.exists(os.path.join(COCO_IMG_DIR, coco_id_to_filename[img_id]))\n]\nprint(f\"Scene/room/nature/vehicle images remaining: {len(valid_ids)}\")\n\n# ---- Sample N_IMAGES image IDs deterministically ----\nrng = np.random.RandomState(SEED)\nsampled_ids = rng.choice(valid_ids, size=N_IMAGES, replace=False).tolist()\n\n# Build ordered list of (filename, first_caption) for the sampled images\ncoco_samples = [\n    (coco_id_to_filename[img_id], coco_captions[img_id][0])\n    for img_id in sampled_ids\n]\n\n# ---- Load MNIST (test set) ----\nmnist = torchvision.datasets.MNIST(\n    root=DATA_ROOT, train=False, download=True\n)\n\n# ---- Sample MNIST indices deterministically ----\nmnist_indices = rng.choice(len(mnist), size=N_IMAGES, replace=False)\n\nprint(f\"MNIST dataset size:  {len(mnist)}\")\nprint(f\"Sampled {N_IMAGES} scene/room/nature/vehicle image-mask pairs.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-3-mask-prep",
   "metadata": {},
   "source": "def prepare_mnist_mask(mnist_image: Image.Image, size=IMAGE_SIZE) -> torch.Tensor:\n    \"\"\"\n    Convert an MNIST digit image into a binary mask for vanilla_second_try.\n    Convention: 1 = keep, 0 = inpaint.\n    The digit silhouette becomes the inpainted (erased) region.\n\n    Returns: torch.Tensor of shape (1, 1, H, W)\n    \"\"\"\n    # Resize 28x28 -> 512x512\n    mask_pil = mnist_image.resize(size, Image.NEAREST)\n    mask_np = np.array(mask_pil)\n\n    # MNIST: digit pixels are bright (>127), background is dark.\n    # We INVERT: digit pixels -> 0 (inpaint), background -> 1 (keep).\n    mask_binary = (mask_np <= 127).astype(np.float32)\n    mask_tensor = torch.from_numpy(mask_binary).unsqueeze(0).unsqueeze(0)\n    return mask_tensor\n\n\ndef prepare_coco_image(path: str, size=IMAGE_SIZE) -> Image.Image:\n    \"\"\"Load a COCO image from disk and resize to the target size.\"\"\"\n    return Image.open(path).convert(\"RGB\").resize(size, Image.LANCZOS)\n\n\ndef apply_mask_for_display(image: Image.Image, mask_tensor: torch.Tensor) -> Image.Image:\n    \"\"\"Black out the inpainted region for visualization.\"\"\"\n    img_np = np.array(image).copy()\n    mask_np = mask_tensor.squeeze().numpy()  # (H, W), 1=keep, 0=inpaint\n    img_np[mask_np == 0] = 0  # black for inpainted region\n    return Image.fromarray(img_np)\n\n\n# ---- Sanity check: display one example ----\nsample_filename, sample_caption = coco_samples[0]\nsample_img = prepare_coco_image(os.path.join(COCO_IMG_DIR, sample_filename))\nsample_mask = prepare_mnist_mask(mnist[mnist_indices[0]][0])\nsample_masked = apply_mask_for_display(sample_img, sample_mask)\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\naxes[0].imshow(sample_img);    axes[0].set_title(\"Original (COCO)\")\naxes[1].imshow(sample_mask.squeeze(), cmap=\"gray\"); axes[1].set_title(\"Mask (1=keep, 0=inpaint)\")\naxes[2].imshow(sample_masked); axes[2].set_title(\"Masked image\")\nfor ax in axes: ax.axis(\"off\")\nplt.suptitle(f'Caption: \"{sample_caption}\"', fontsize=10, y=1.01)\nplt.tight_layout(); plt.show()\n\ninpaint_frac = (sample_mask == 0).float().mean().item() * 100\nprint(f\"Inpainted region: {inpaint_frac:.1f}% of image\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-4-model",
   "metadata": {},
   "source": "# ---- Import model and inpainting function from vanilla_second_try.py ----\nsys.path.insert(0, os.path.abspath(\".\"))\nfrom vanilla_second_try import load_vanilla_model, ddpm_inpaint\n\ndevice = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\nprint(\"Loading model...\")\npipe = load_vanilla_model(device)\npipe.set_progress_bar_config(disable=True)  # suppress inner per-step bar\nprint(\"Model loaded.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-5-inpaint-loop",
   "metadata": {},
   "source": "# ---- Inpainting loop with disk-based resume support ----\n\n# Check for existing checkpoint\nstart_idx = 0\nif os.path.exists(CHECKPOINT_PATH):\n    with open(CHECKPOINT_PATH, \"r\") as f:\n        checkpoint = json.load(f)\n    start_idx = checkpoint.get(\"completed\", 0)\n    print(f\"Resuming from image {start_idx}/{N_IMAGES}\")\n\nfor i in tqdm(range(start_idx, N_IMAGES), initial=start_idx,\n              total=N_IMAGES, desc=\"Inpainting\"):\n    # Load COCO image from disk\n    filename, prompt = coco_samples[i]\n    img_pil = prepare_coco_image(os.path.join(COCO_IMG_DIR, filename))\n\n    # Prepare mask\n    mask_tensor = prepare_mnist_mask(mnist[mnist_indices[i]][0])\n\n    # Run DDPM inpainting (prompt is the COCO caption verbatim)\n    inpainted_pil = ddpm_inpaint(\n        pipe=pipe,\n        image=img_pil,\n        mask=mask_tensor,\n        prompt=prompt,\n        steps=STEPS,\n        guidance_scale=GUIDANCE_SCALE,\n        seed=SEED + i,\n    )\n\n    # Save to disk\n    img_pil.save(os.path.join(RESULTS_DIR, \"originals\", f\"{i:04d}.png\"))\n    inpainted_pil.save(os.path.join(RESULTS_DIR, \"inpainted\", f\"{i:04d}.png\"))\n    torch.save(mask_tensor, os.path.join(RESULTS_DIR, \"masks\", f\"{i:04d}.pt\"))\n    with open(os.path.join(RESULTS_DIR, \"originals\", f\"{i:04d}.json\"), \"w\") as f:\n        json.dump({\"caption\": prompt}, f)\n\n    # Update checkpoint\n    with open(CHECKPOINT_PATH, \"w\") as f:\n        json.dump({\"completed\": i + 1, \"last_prompt\": prompt}, f)\n\nprint(f\"Done. Inpainted {N_IMAGES} images.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-6-metrics",
   "metadata": {},
   "source": "# ---- Compute metrics on the masked (inpainted) region only ----\n\nlpips_model = lpips.LPIPS(net=\"alex\").to(device)\nlpips_model.eval()\n\n\ndef compute_masked_metrics(\n    original: Image.Image,\n    inpainted: Image.Image,\n    mask_tensor: torch.Tensor,\n) -> dict:\n    \"\"\"\n    Compute SSIM, PSNR, LPIPS only on the inpainted region.\n    mask_tensor: (1,1,H,W), 1=keep, 0=inpaint.\n    \"\"\"\n    orig_np = np.array(original).astype(np.float64)\n    inp_np  = np.array(inpainted).astype(np.float64)\n    mask_np = mask_tensor.squeeze().numpy()\n    inpaint_mask = (mask_np == 0)\n\n    if not inpaint_mask.any():\n        return {\"ssim\": 1.0, \"psnr\": float(\"inf\"), \"lpips\": 0.0}\n\n    # ---- Bounding box of inpainted region ----\n    rows = np.any(inpaint_mask, axis=1)\n    cols = np.any(inpaint_mask, axis=0)\n    rmin, rmax = np.where(rows)[0][[0, -1]]\n    cmin, cmax = np.where(cols)[0][[0, -1]]\n\n    crop_orig = orig_np[rmin:rmax + 1, cmin:cmax + 1]\n    crop_inp  = inp_np[rmin:rmax + 1, cmin:cmax + 1]\n\n    # ---- PSNR: only on inpainted pixels ----\n    masked_orig = orig_np[inpaint_mask]\n    masked_inp  = inp_np[inpaint_mask]\n    mse = np.mean((masked_orig - masked_inp) ** 2)\n    psnr_val = 10.0 * np.log10(255.0 ** 2 / mse) if mse > 0 else float(\"inf\")\n\n    # ---- SSIM: on bounding-box crop ----\n    min_dim = min(crop_orig.shape[0], crop_orig.shape[1])\n    win_size = min(7, min_dim if min_dim % 2 == 1 else min_dim - 1)\n    win_size = max(win_size, 3)\n\n    ssim_val = ssim_fn(\n        crop_orig, crop_inp,\n        data_range=255.0, channel_axis=2, win_size=win_size\n    )\n\n    # ---- LPIPS: on bounding-box crop (avoids black-region artifacts) ----\n    crop_orig_f = crop_orig.astype(np.float32)\n    crop_inp_f  = crop_inp.astype(np.float32)\n\n    orig_t = torch.from_numpy(crop_orig_f).permute(2, 0, 1).unsqueeze(0) / 127.5 - 1.0\n    inp_t  = torch.from_numpy(crop_inp_f).permute(2, 0, 1).unsqueeze(0) / 127.5 - 1.0\n\n    with torch.no_grad():\n        lpips_val = lpips_model(orig_t.to(device), inp_t.to(device)).item()\n\n    return {\"ssim\": ssim_val, \"psnr\": psnr_val, \"lpips\": lpips_val}\n\n\n# ---- Load results from disk and compute metrics ----\nresults = []\nfor i in tqdm(range(N_IMAGES), desc=\"Computing metrics\"):\n    orig_path    = os.path.join(RESULTS_DIR, \"originals\", f\"{i:04d}.png\")\n    inp_path     = os.path.join(RESULTS_DIR, \"inpainted\", f\"{i:04d}.png\")\n    mask_path    = os.path.join(RESULTS_DIR, \"masks\",     f\"{i:04d}.pt\")\n    caption_path = os.path.join(RESULTS_DIR, \"originals\", f\"{i:04d}.json\")\n\n    if not all(os.path.exists(p) for p in [orig_path, inp_path, mask_path]):\n        print(f\"Stopping at image {i} (files not found). Run inpainting loop first.\")\n        break\n\n    orig_img = Image.open(orig_path)\n    inp_img  = Image.open(inp_path)\n    mask_t   = torch.load(mask_path, weights_only=True)\n\n    caption = \"\"\n    if os.path.exists(caption_path):\n        with open(caption_path) as f:\n            caption = json.load(f).get(\"caption\", \"\")\n\n    metrics = compute_masked_metrics(orig_img, inp_img, mask_t)\n    results.append({\n        \"idx\": i, \"original\": orig_img, \"inpainted\": inp_img,\n        \"mask\": mask_t, \"caption\": caption, **metrics\n    })\n\nprint(f\"Computed metrics for {len(results)} images.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-7-summary",
   "metadata": {},
   "source": [
    "# ---- Aggregate statistics and KDE distributions ----\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "ssim_vals  = np.array([r[\"ssim\"]  for r in results])\n",
    "psnr_vals  = np.array([r[\"psnr\"]  for r in results])\n",
    "lpips_vals = np.array([r[\"lpips\"] for r in results])\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(f\"Results over {len(results)} images (inpainted region only)\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"{'Metric':<10} {'Mean':>10} {'Median':>10} {'Std':>10}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'SSIM':<10} {ssim_vals.mean():>10.4f} {np.median(ssim_vals):>10.4f} {ssim_vals.std():>10.4f}\")\n",
    "print(f\"{'PSNR':<10} {psnr_vals.mean():>10.2f} {np.median(psnr_vals):>10.2f} {psnr_vals.std():>10.2f}\")\n",
    "print(f\"{'LPIPS':<10} {lpips_vals.mean():>10.4f} {np.median(lpips_vals):>10.4f} {lpips_vals.std():>10.4f}\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "def plot_kde(ax, values, color, xlabel, title):\n",
    "    kde = gaussian_kde(values, bw_method=\"scott\")\n",
    "    x = np.linspace(values.min() - 0.05 * np.ptp(values),\n",
    "                    values.max() + 0.05 * np.ptp(values), 500)\n",
    "    y = kde(x)\n",
    "    ax.plot(x, y, color=color, linewidth=2)\n",
    "    ax.fill_between(x, y, alpha=0.25, color=color)\n",
    "    ax.axvline(values.mean(),   color=color, linestyle=\"--\", linewidth=1.2, label=f\"mean={values.mean():.3f}\")\n",
    "    ax.axvline(np.median(values), color=\"black\", linestyle=\":\",  linewidth=1.2, label=f\"median={np.median(values):.3f}\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "plot_kde(axes[0], ssim_vals,  \"steelblue\",  \"SSIM\",           \"SSIM Distribution\")\n",
    "plot_kde(axes[1], psnr_vals,  \"darkorange\", \"PSNR (dB)\",      \"PSNR Distribution\")\n",
    "plot_kde(axes[2], lpips_vals, \"seagreen\",   \"LPIPS (Lower is better)\", \"LPIPS Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, \"metric_distributions.png\"), dpi=150)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-8-top10-best",
   "metadata": {},
   "source": "# ---- Top 10 BEST (highest SSIM = most similar reconstruction) ----\nsorted_by_ssim = sorted(results, key=lambda r: r[\"ssim\"], reverse=True)\ntop10_best = sorted_by_ssim[:10]\n\nfig, axes = plt.subplots(10, 3, figsize=(15, 50))\nfig.suptitle(\"Top 10 BEST Inpainting Results (highest SSIM)\", fontsize=16, y=1.0)\n\nfor row, r in enumerate(top10_best):\n    masked_vis = apply_mask_for_display(r[\"original\"], r[\"mask\"])\n\n    axes[row, 0].imshow(r[\"original\"])\n    axes[row, 0].set_title(\"Original\" if row == 0 else \"\")\n    axes[row, 0].set_ylabel(\n        f\"#{r['idx']}\\nSSIM={r['ssim']:.3f}\\nPSNR={r['psnr']:.1f}\\nLPIPS={r['lpips']:.3f}\",\n        fontsize=9, rotation=0, labelpad=70, va=\"center\"\n    )\n\n    axes[row, 1].imshow(masked_vis)\n    axes[row, 1].set_title(\"Masked\" if row == 0 else \"\")\n    axes[row, 1].set_xlabel(f'\"{r[\"caption\"]}\"', fontsize=8, labelpad=6)\n\n    axes[row, 2].imshow(r[\"inpainted\"])\n    axes[row, 2].set_title(\"Inpainted\" if row == 0 else \"\")\n\n    for col in range(3):\n        axes[row, col].axis(\"off\")\n    # Re-enable x-axis label on the masked column (axis(\"off\") hides it)\n    axes[row, 1].xaxis.set_visible(True)\n    axes[row, 1].tick_params(bottom=False, labelbottom=True)\n    axes[row, 1].set_xticks([])\n\nplt.tight_layout()\nplt.savefig(os.path.join(RESULTS_DIR, \"top10_best.png\"), dpi=150, bbox_inches=\"tight\")\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-9-top10-worst",
   "metadata": {},
   "source": "# ---- Top 10 WORST (lowest SSIM = least similar reconstruction) ----\ntop10_worst = sorted_by_ssim[-10:]\n\nfig, axes = plt.subplots(10, 3, figsize=(15, 50))\nfig.suptitle(\"Top 10 WORST Inpainting Results (lowest SSIM)\", fontsize=16, y=1.0)\n\nfor row, r in enumerate(top10_worst):\n    masked_vis = apply_mask_for_display(r[\"original\"], r[\"mask\"])\n\n    axes[row, 0].imshow(r[\"original\"])\n    axes[row, 0].set_title(\"Original\" if row == 0 else \"\")\n    axes[row, 0].set_ylabel(\n        f\"#{r['idx']}\\nSSIM={r['ssim']:.3f}\\nPSNR={r['psnr']:.1f}\\nLPIPS={r['lpips']:.3f}\",\n        fontsize=9, rotation=0, labelpad=70, va=\"center\"\n    )\n\n    axes[row, 1].imshow(masked_vis)\n    axes[row, 1].set_title(\"Masked\" if row == 0 else \"\")\n    axes[row, 1].set_xlabel(f'\"{r[\"caption\"]}\"', fontsize=8, labelpad=6)\n\n    axes[row, 2].imshow(r[\"inpainted\"])\n    axes[row, 2].set_title(\"Inpainted\" if row == 0 else \"\")\n\n    for col in range(3):\n        axes[row, col].axis(\"off\")\n    # Re-enable x-axis label on the masked column (axis(\"off\") hides it)\n    axes[row, 1].xaxis.set_visible(True)\n    axes[row, 1].tick_params(bottom=False, labelbottom=True)\n    axes[row, 1].set_xticks([])\n\nplt.tight_layout()\nplt.savefig(os.path.join(RESULTS_DIR, \"top10_worst.png\"), dpi=150, bbox_inches=\"tight\")\nplt.show()",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
